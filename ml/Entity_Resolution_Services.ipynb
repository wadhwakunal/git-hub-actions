{
	"cells": [{
			"cell_type": "code",
			"execution_count": 2,
			"id": "15f1a9ff-311b-4b17-8291-00200bd6a671",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Copyright 2023 Google LLC\n",
				"#\n",
				"# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
				"# you may not use this file except in compliance with the License.\n",
				"# You may obtain a copy of the License at\n",
				"#\n",
				"#     https://urldefense.com/v3/__https://www.apache.org/licenses/LICENSE-2.0*5Cn__;JQ!!KEc8uF_xo8-al5zF!TFxM5JIZ9RmphkPHNnVGObbMz3_4wX82ie0o_wIrJ31_owKC5da5eEro18u5ec7Nt-gk-kJOCkDyjaE5$ ",
				"#\n",
				"# Unless required by applicable law or agreed to in writing, software\n",
				"# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
				"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
				"# See the License for the specific language governing permissions and\n",
				"# limitations under the License."
			]
		},
		{
			"cell_type": "markdown",
			"id": "53c26d1f-6992-4e34-a090-3ab05b33bc86",
			"metadata": {},
			"source": [
				"## Installation\n",
				"\n",
				"1. Install the necessary python Packages.  Note these workbenches come with pre-installed libraries so we might have to investigate the libraries we would need to install on a service\n",
				"2. Set the project, region, and bucket_uri"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"id": "b470a8b9-01eb-4792-b420-e76472f88c45",
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"3.10.11\n"
				]
			}],
			"source": [
				"# Need to install Python 3.8> for all the Langchain Libs to work\n",
				"from platform import python_version\n",
				"print(python_version())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"id": "f8038f3e-aed1-420b-9edf-46b639a2dccd",
			"metadata": {
				"tags": []
			},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"Note: you may need to restart the kernel to use updated packages.\n",
					"Note: you may need to restart the kernel to use updated packages.\n",
					"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
					"google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.11.1 which is incompatible.\u001b[0m\u001b[31m\n",
					"\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
					"Note: you may need to restart the kernel to use updated packages.\n",
					"Note: you may need to restart the kernel to use updated packages.\n"
				]
			}],
			"source": [
				"# Install the packages\n",
				"!pip install --upgrade --quiet google-cloud-aiplatform \\\n",
				"                        google-cloud-storage \\\n",
				"                        'google-cloud-bigquery[pandas]' \\\n",
				"                        typo \\\n",
				"                        google-cloud-storage\n",
				"\n",
				"!pip install python-Levenshtein --quiet\n",
				"!pip install --quiet pandas_gbq\n",
				"!pip install --quiet langchain\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"id": "364b2128-0d90-4601-9d05-527c06a0b6c2",
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"Collecting flake8\n",
					"  Downloading flake8-6.0.0-py2.py3-none-any.whl (57 kB)\n",
					"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
					"\u001b[?25hCollecting pycodestyle_magic\n",
					"  Downloading pycodestyle_magic-0.5-py2.py3-none-any.whl (9.5 kB)\n",
					"Collecting mccabe<0.8.0,>=0.7.0 (from flake8)\n",
					"  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
					"Collecting pycodestyle<2.11.0,>=2.10.0 (from flake8)\n",
					"  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
					"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
					"\u001b[?25hCollecting pyflakes<3.1.0,>=3.0.0 (from flake8)\n",
					"  Downloading pyflakes-3.0.1-py2.py3-none-any.whl (62 kB)\n",
					"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
					"\u001b[?25hInstalling collected packages: pycodestyle_magic, pyflakes, pycodestyle, mccabe, flake8\n",
					"Successfully installed flake8-6.0.0 mccabe-0.7.0 pycodestyle-2.10.0 pycodestyle_magic-0.5 pyflakes-3.0.1\n"
				]
			}],
			"source": [
				"!pip install flake8 pycodestyle_magic"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"id": "718c5a67-9206-43cd-a7b6-8cf00edde303",
			"metadata": {},
			"outputs": [],
			"source": [
				"PROJECT_ID = \"whisper-test-378918\"\n",
				"\n",
				"# Set the project id\n",
				"# ! gcloud config set project {PROJECT_ID}\n",
				"REGION = \"us-central1\"  # @param {type: \"string\"}\n",
				"BUCKET_URI = \"gs://whisper-test-fintech-demo\"  # @param {type:\"string\"}"
			]
		},
		{
			"cell_type": "markdown",
			"id": "6e603a46-c2ac-4abe-804a-a79c090a8691",
			"metadata": {},
			"source": [
				"## Entity Extractor"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"id": "01c871e5-e8a0-4ba1-8ece-e3d158f5fcae",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Import necessary libraries\n",
				"from google.cloud import bigquery\n",
				"from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Type, Tuple, Dict\n",
				"import json\n",
				"import numpy as np\n",
				"import pandas as pd\n",
				"import pprint as pp\n",
				"\n",
				"\n",
				"# BigQueryFetcher class\n",
				"# Class for taking queries and creating dataframes or cleaned dictionaries (No Null keys/values)\n",
				"class BigQueryFetcher:\n",
				"    # Initialize BigQueryFetcher with the Google Cloud project ID\n",
				"    def __init__(self, project_id: str):\n",
				"        self.client = bigquery.Client(project=project_id)\n",
				"\n",
				"    # Run the provided SQL query using BigQuery\n",
				"    # and return the results as a dataframe\n",
				"    def query_to_df(self, query: str) -> pd.DataFrame:\n",
				"        query_job = self.client.query(query)\n",
				"        rows = query_job.result()\n",
				"        df = query_job.to_dataframe()\n",
				"\n",
				"        return df\n",
				"\n",
				"    def remove_null_keys_values(self, df: pd.DataFrame,\n",
				"                                key: List[str]) -> Dict:\n",
				"        df = df.where(pd.notnull(df), None)  # Replace pandas NaN with None\n",
				"        # Convert DataFrame to list of dictionaries\n",
				"        records = df.to_dict(orient='records')\n",
				"\n",
				"        cleaned_records = {}\n",
				"        for record in records:\n",
				"            # For each record, create a new dictionary\n",
				"            # that only includes keys with non-null values\n",
				"            cleaned_record = {k: v for k, v in record.items() if v is not None}\n",
				"            lei = cleaned_record.pop(key, None)\n",
				"            if lei is not None:\n",
				"                cleaned_records[lei] = cleaned_record\n",
				"\n",
				"        return cleaned_records\n",
				"\n",
				"    # Run the provided query, then clean the resulting data\n",
				"    def df_to_clean_dict(self, query: str, key: List[str]) -> Dict:\n",
				"        df = self.query_to_df(query)\n",
				"        cleaned_data = self.remove_null_keys_values(df, key)\n",
				"        return cleaned_data"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"id": "42da7409-f9d3-4d2d-ae23-2fc57b3bbfcd",
			"metadata": {},
			"outputs": [],
			"source": [
				"#Subject to change if we want the datasource to come from a GCS bucket or a client request\n",
				"\n",
				"bigquery_fetcher = BigQueryFetcher(PROJECT_ID)\n",
				"\n",
				"##SQL query to retrive the transactions with reference\n",
				"# Update the table & where clause based on the transactions with typos dataset\n",
				"query = '''\n",
				"SELECT APPLREF, REMITTANCE_INFO FROM `whisper-test-378918.fintech_dummy_data.transaction_typos_demo` \n",
				"where remittance_info not in ('PAY INVOICE', 'FOR SERVICES','OAT - OWN ACCOUNT TRANSFER  SERVICE','WITHDRAWAL TO BANK ACCOUNT') and \n",
				"remittance_info not like '%/BEN None%' and remittance_info like '%//BEN%'\n",
				"\n",
				"ORDER BY 2 desc;\n",
				"'''\n",
				"\n",
				"\n",
				"transaction_demo = bigquery_fetcher.query_to_df(query)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"id": "7b0a317f-54af-42c6-840b-9d7bae22a07d",
			"metadata": {},
			"outputs": [{
				"data": {
					"text/html": [
						"<div>\n",
						"<style scoped>\n",
						"    .dataframe tbody tr th:only-of-type {\n",
						"        vertical-align: middle;\n",
						"    }\n",
						"\n",
						"    .dataframe tbody tr th {\n",
						"        vertical-align: top;\n",
						"    }\n",
						"\n",
						"    .dataframe thead th {\n",
						"        text-align: right;\n",
						"    }\n",
						"</style>\n",
						"<table border=\"1\" class=\"dataframe\">\n",
						"  <thead>\n",
						"    <tr style=\"text-align: right;\">\n",
						"      <th></th>\n",
						"      <th>APPLREF</th>\n",
						"      <th>REMITTANCE_INFO</th>\n",
						"    </tr>\n",
						"  </thead>\n",
						"  <tbody>\n",
						"    <tr>\n",
						"      <th>0</th>\n",
						"      <td>20230601EQHUJAXMNB</td>\n",
						"      <td>/RFB/9998477510811 //EUR 8.30237381948875/GBP ...</td>\n",
						"    </tr>\n",
						"    <tr>\n",
						"      <th>1</th>\n",
						"      <td>20230601FYCVRMQXFQ</td>\n",
						"      <td>/RFB/9997320282130 //SGD 10.970051323U14433/GB...</td>\n",
						"    </tr>\n",
						"    <tr>\n",
						"      <th>2</th>\n",
						"      <td>20230601JYICKWTCZV</td>\n",
						"      <td>/RFB/9995885640522 //EUR 2.418595817079784/GBP...</td>\n",
						"    </tr>\n",
						"    <tr>\n",
						"      <th>3</th>\n",
						"      <td>20230601OOJEHZOCGR</td>\n",
						"      <td>/RFB/9993714374527 //EUR 5.00001966544501/GBP ...</td>\n",
						"    </tr>\n",
						"    <tr>\n",
						"      <th>4</th>\n",
						"      <td>20230601RHQUJTDMMY</td>\n",
						"      <td>/RFB/9992740692343 //GBP 10.5824 01151471869/U...</td>\n",
						"    </tr>\n",
						"  </tbody>\n",
						"</table>\n",
						"</div>"
					],
					"text/plain": [
						"              APPLREF                                    REMITTANCE_INFO\n",
						"0  20230601EQHUJAXMNB  /RFB/9998477510811 //EUR 8.30237381948875/GBP ...\n",
						"1  20230601FYCVRMQXFQ  /RFB/9997320282130 //SGD 10.970051323U14433/GB...\n",
						"2  20230601JYICKWTCZV  /RFB/9995885640522 //EUR 2.418595817079784/GBP...\n",
						"3  20230601OOJEHZOCGR  /RFB/9993714374527 //EUR 5.00001966544501/GBP ...\n",
						"4  20230601RHQUJTDMMY  /RFB/9992740692343 //GBP 10.5824 01151471869/U..."
					]
				},
				"execution_count": 10,
				"metadata": {},
				"output_type": "execute_result"
			}],
			"source": [
				"transaction_demo.head()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 13,
			"id": "c9c5d068-71bf-4ac9-a767-046b9963bee2",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Used to Manage GCP quota limits for vertex models\n",
				"#https://urldefense.com/v3/__https://cloud.google.com/vertex-ai/docs/quotas*request_quotas*5Cn__;IyU!!KEc8uF_xo8-al5zF!TFxM5JIZ9RmphkPHNnVGObbMz3_4wX82ie0o_wIrJ31_owKC5da5eEro18u5ec7Nt-gk-kJOCnqPlnlN$ ",
				"\n",
				"def rate_limit(max_per_minute: int):\n",
				"    period = 60 / max_per_minute\n",
				"    print(\"Waiting\")\n",
				"    while True:\n",
				"        before = time.time()\n",
				"        yield\n",
				"        after = time.time()\n",
				"        elapsed = after - before\n",
				"        sleep_time = max(0, period - elapsed)\n",
				"        if sleep_time > 0:\n",
				"            print(\".\", end=\"\")\n",
				"            time.sleep(sleep_time)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"id": "0510888d-9e2f-4ada-811e-b1a7c2b05b47",
			"metadata": {},
			"outputs": [],
			"source": [
				"from langchain.llms import VertexAI\n",
				"from langchain import PromptTemplate, LLMChain\n",
				"from tqdm import tqdm\n",
				"from typing import List, Tuple, NamedTuple, Dict\n",
				"import pandas as pd\n",
				"import time\n",
				"from dataclasses import dataclass\n",
				"import threading\n",
				"from queue import Queue\n",
				"\n",
				"from langchain.output_parsers import PydanticOutputParser\n",
				"from pydantic import BaseModel, Field\n",
				"\n",
				"\n",
				"# Define your  data structure that\n",
				"# the entity extraction service will return\n",
				"@dataclass\n",
				"class Extraction:\n",
				"    document_id: str\n",
				"    entity: Dict\n",
				"\n",
				"    \n",
				"# Define your desired data structure for the returned json.\n",
				"class entity_name(BaseModel):\n",
				"    entity_name: str = Field(\n",
				"        description=\"\"\"\n",
				"        Entity name that is extracted from string.\n",
				"        The name can be in many languages\n",
				"        \"\"\"\n",
				"    )\n",
				"\n",
				"\n",
				"class EntityExtraction:\n",
				"    \"\"\"A class to summarize documents with LLM.\"\"\"\n",
				"    def __init__(self, requests_per_minute: int):\n",
				"        self._llm = VertexAI(\n",
				"            model_name=\"text-bison@001\",\n",
				"            max_output_tokens=1024,\n",
				"            temperature=0,\n",
				"            top_p=0.8,\n",
				"            top_k=40,\n",
				"            verbose=True,\n",
				"        )\n",
				"        self.entity_extraction_preamble = '''\n",
				"        As an AI assistant, your task involves meticulously reviewing text and\n",
				"        discerning the important details. In this case, your focus is to\n",
				"        identify the entity names present in the input text.\n",
				"        Create a JSON object with the extracted company names.\n",
				"        Follow the format provided in the instructions below.\n",
				"        In instances where the text doesn't include any company\n",
				"        or individual names, simply return 'none'. Note that\n",
				"        some texts may contain the abbreviation 'BEN',\n",
				"        which stands for 'beneficiary'.\n",
				"\n",
				"        Format instructions: {format_instructions}\n",
				"\n",
				"        Sample:\n",
				"        Input: {input_example}\n",
				"        Output:\n",
				"        '''\n",
				"        self.parser = PydanticOutputParser(pydantic_object=entity_name)\n",
				"        self._prompt = PromptTemplate(\n",
				"                            input_variables=[\"input_example\"],\n",
				"                            template=self.entity_extraction_preamble,\n",
				"                            partial_variables={\"format_instructions\":\n",
				"                                               self.parser.get_format_instructions()})\n",
				"        self.llm_chain = LLMChain(prompt=self._prompt, llm=self._llm)\n",
				"        self.requests_per_minute = requests_per_minute\n",
				"\n",
				"    def _extract_entities_thread(self, dataframe: pd.DataFrame, document_id: str, document_string: str,  extractions: Queue, documents_not_extracted: Queue) -> List[Tuple[int, str]]:\n",
				"        \"\"\"Extract entities from dataframe using LLM\n",
				"        and return a list of namedtuples.\n",
				"\n",
				"        Parameters:\n",
				"            dataframe (pd.DataFrame): The dataframe to extract from.\n",
				"            document_id (str): The column id.\n",
				"            document_string (str): The document string to analyze.\n",
				"\n",
				"        Returns:\n",
				"            List[Tuple[int, str]]:\n",
				"            A list of namedtuples where each tuple contains a column id and an entity.\n",
				"        \"\"\"\n",
				"        limiter = rate_limit(self.requests_per_minute)\n",
				"        # Extraction = namedtuple('Extraction',\n",
				"        # [document_id, document_string])\n",
				"\n",
				"        for idx, row in tqdm(dataframe.iterrows(),\n",
				"                             total=dataframe.shape[0],\n",
				"                             desc=\"Extracting entities\"):\n",
				"            try:\n",
				"                string_to_extract = row[document_string]\n",
				"                column_id = row[document_id]\n",
				"                # print(self.llm_chain.prompt.format(input_example=string_to_extract))\n",
				"                entity = self.llm_chain.run(string_to_extract)\n",
				"                # print(entity)\n",
				"                next(limiter)\n",
				"                parsed_entity = self.parser.parse(entity)\n",
				"                extraction = Extraction(column_id, parsed_entity)\n",
				"                # extractions.append(extraction)\n",
				"                extractions.put(extraction)  # Use the Queue's put method to add to the extractions\n",
				"\n",
				"            except Exception as e:\n",
				"                print(f\"Failed to extract entity for row {idx}: {e}\")\n",
				"                document_not_extracted = column_id\n",
				"        return extractions, document_not_extracted\n",
				"    \n",
				"    \n",
				"    def extract_entities(self, dataframe: pd.DataFrame, document_id: str, document_string: str):\n",
				"        extractions = Queue()  # Use a Queue to store the extractions\n",
				"        documents_not_extracted  = Queue()\n",
				"        thread = threading.Thread(target=self._extract_entities_thread, args=(dataframe, document_id, document_string, extractions, documents_not_extracted))\n",
				"        thread.start()\n",
				"        return extractions, documents_not_extracted, thread  # Return the Queue and the Thread"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 14,
			"id": "f705918d-39c3-4423-9787-f6d4150b5af6",
			"metadata": {},
			"outputs": [{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:   1%|          | 1/100 [00:00<00:58,  1.69it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Waiting\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:   2%|▏         | 2/100 [00:01<01:21,  1.20it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						".."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:   4%|▍         | 4/100 [00:03<01:30,  1.06it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						".."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:   5%|▌         | 5/100 [00:04<01:31,  1.04it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:   6%|▌         | 6/100 [00:05<01:31,  1.02it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:   7%|▋         | 7/100 [00:06<01:31,  1.02it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:   8%|▊         | 8/100 [00:07<01:31,  1.01it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:   9%|▉         | 9/100 [00:08<01:30,  1.01it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  10%|█         | 10/100 [00:09<01:29,  1.00it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  11%|█         | 11/100 [00:10<01:28,  1.00it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  12%|█▏        | 12/100 [00:11<01:27,  1.00it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  13%|█▎        | 13/100 [00:12<01:26,  1.00it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  14%|█▍        | 14/100 [00:13<01:25,  1.00it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  15%|█▌        | 15/100 [00:14<01:24,  1.00it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  16%|█▌        | 16/100 [00:15<01:23,  1.00it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  17%|█▋        | 17/100 [00:16<01:23,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  18%|█▊        | 18/100 [00:17<01:22,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  19%|█▉        | 19/100 [00:18<01:21,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  20%|██        | 20/100 [00:19<01:20,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  21%|██        | 21/100 [00:20<01:19,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  22%|██▏       | 22/100 [00:21<01:18,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  23%|██▎       | 23/100 [00:22<01:17,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  24%|██▍       | 24/100 [00:23<01:16,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  25%|██▌       | 25/100 [00:24<01:15,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  26%|██▌       | 26/100 [00:25<01:14,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  27%|██▋       | 27/100 [00:26<01:13,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  28%|██▊       | 28/100 [00:27<01:12,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  29%|██▉       | 29/100 [00:28<01:11,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  30%|███       | 30/100 [00:29<01:10,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  31%|███       | 31/100 [00:30<01:09,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  32%|███▏      | 32/100 [00:31<01:08,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  33%|███▎      | 33/100 [00:32<01:07,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  34%|███▍      | 34/100 [00:33<01:06,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  35%|███▌      | 35/100 [00:34<01:05,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  36%|███▌      | 36/100 [00:35<01:04,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  37%|███▋      | 37/100 [00:36<01:03,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  38%|███▊      | 38/100 [00:37<01:02,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  40%|████      | 40/100 [00:39<01:00,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						".."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  41%|████      | 41/100 [00:40<00:59,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  42%|████▏     | 42/100 [00:41<00:58,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  43%|████▎     | 43/100 [00:42<00:57,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Failed to extract entity for row 42: Failed to parse entity_name from completion . Got: Expecting value: line 1 column 1 (char 0)\n",
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  44%|████▍     | 44/100 [00:43<00:56,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  45%|████▌     | 45/100 [00:44<00:55,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  46%|████▌     | 46/100 [00:45<00:54,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  47%|████▋     | 47/100 [00:46<00:53,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  48%|████▊     | 48/100 [00:47<00:52,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  49%|████▉     | 49/100 [00:48<00:51,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  51%|█████     | 51/100 [00:50<00:49,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						".."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  52%|█████▏    | 52/100 [00:51<00:48,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  53%|█████▎    | 53/100 [00:52<00:47,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  54%|█████▍    | 54/100 [00:53<00:46,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  55%|█████▌    | 55/100 [00:54<00:45,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  56%|█████▌    | 56/100 [00:55<00:44,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  57%|█████▋    | 57/100 [00:56<00:43,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  58%|█████▊    | 58/100 [00:57<00:42,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  59%|█████▉    | 59/100 [00:58<00:41,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  60%|██████    | 60/100 [00:59<00:40,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  61%|██████    | 61/100 [01:00<00:39,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  62%|██████▏   | 62/100 [01:01<00:38,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  63%|██████▎   | 63/100 [01:02<00:37,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  64%|██████▍   | 64/100 [01:03<00:36,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  65%|██████▌   | 65/100 [01:04<00:35,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  66%|██████▌   | 66/100 [01:05<00:34,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  67%|██████▋   | 67/100 [01:06<00:33,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  68%|██████▊   | 68/100 [01:07<00:32,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  69%|██████▉   | 69/100 [01:08<00:31,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  70%|███████   | 70/100 [01:09<00:30,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  71%|███████   | 71/100 [01:10<00:29,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  72%|███████▏  | 72/100 [01:11<00:28,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  73%|███████▎  | 73/100 [01:12<00:27,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  74%|███████▍  | 74/100 [01:13<00:26,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  75%|███████▌  | 75/100 [01:14<00:25,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  76%|███████▌  | 76/100 [01:15<00:24,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  77%|███████▋  | 77/100 [01:16<00:23,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  78%|███████▊  | 78/100 [01:17<00:22,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  79%|███████▉  | 79/100 [01:18<00:21,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  80%|████████  | 80/100 [01:19<00:20,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  81%|████████  | 81/100 [01:20<00:19,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  82%|████████▏ | 82/100 [01:21<00:18,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  83%|████████▎ | 83/100 [01:22<00:17,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  84%|████████▍ | 84/100 [01:23<00:16,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  85%|████████▌ | 85/100 [01:24<00:15,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  86%|████████▌ | 86/100 [01:25<00:14,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  87%|████████▋ | 87/100 [01:26<00:13,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  88%|████████▊ | 88/100 [01:27<00:12,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  89%|████████▉ | 89/100 [01:28<00:11,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  90%|█████████ | 90/100 [01:29<00:10,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  91%|█████████ | 91/100 [01:30<00:09,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  92%|█████████▏| 92/100 [01:31<00:08,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  93%|█████████▎| 93/100 [01:32<00:07,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  94%|█████████▍| 94/100 [01:33<00:06,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  95%|█████████▌| 95/100 [01:34<00:05,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  96%|█████████▌| 96/100 [01:35<00:04,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  97%|█████████▋| 97/100 [01:36<00:03,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  98%|█████████▊| 98/100 [01:37<00:02,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities:  99%|█████████▉| 99/100 [01:38<00:01,  1.00s/it]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Extracting entities: 100%|██████████| 100/100 [01:39<00:00,  1.00it/s]\n"
					]
				}
			],
			"source": [
				"entity_extraction = EntityExtraction(60)\n",
				"\n",
				"# Assumes that the document_id provided is unique to each row\n",
				"# For demo purposes, only extracts 100 records\n",
				"extractions, documents_not_extracted, thread = entity_extraction.extract_entities(transaction_demo.head(100), 'APPLREF', 'REMITTANCE_INFO')\n",
				"\n",
				"extractions_list = []\n",
				"documents_not_extracted_list = [] # Inputs that failed on extraction step\n",
				"    \n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 15,
			"id": "39dff812-5960-440a-9214-57c6a464a623",
			"metadata": {},
			"outputs": [],
			"source": [
				"# And if you want to ensure that the thread has finished, you can do:\n",
				"thread.join()\n",
				"\n",
				"while not extractions.empty():\n",
				"    extractions_list.append(extractions.get())\n",
				"\n",
				"\n",
				"while not documents_not_extracted.empty():\n",
				"    documents_not_extracted_list.append(documents_not_extracted.get())\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 16,
			"id": "d026b0c8-7639-4598-b63f-1370f5bdeda2",
			"metadata": {},
			"outputs": [{
				"data": {
					"text/plain": [
						"[Extraction(document_id='20230601EQHUJAXMNB', entity=entity_name(entity_name='FIT ACTIVE BODY LTD')),\n",
						" Extraction(document_id='20230601FYCVRMQXFQ', entity=entity_name(entity_name='THE FITNESS EXCHANGE AT THOMAS MORE SQUARE LIMITED')),\n",
						" Extraction(document_id='20230601JYICKWTCZV', entity=entity_name(entity_name='BEN SIMMONS OF CHESHIRE LTD')),\n",
						" Extraction(document_id='20230601OOJEHZOCGR', entity=entity_name(entity_name='SKYANGELS AIR AMBULANCE GLOBAL LTD')),\n",
						" Extraction(document_id='20230601RHQUJTDMMY', entity=entity_name(entity_name='Megan Vargas'))]"
					]
				},
				"execution_count": 16,
				"metadata": {},
				"output_type": "execute_result"
			}],
			"source": [
				"extractions_list[0:5]"
			]
		},
		{
			"cell_type": "markdown",
			"id": "aeb34d62-074b-4e8c-a68b-28a22e874302",
			"metadata": {},
			"source": [
				"## Create the Embeddings\n",
				"\n",
				"1. Create a class to generate the embeddings. The class will process the list of structs in chunks of 5 to comply with the API limitations.\n",
				"2. Save the embeddings to a bucket to be used for the index with matching engine.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 210,
			"id": "66e497c3-249c-40be-aef7-7ff5546ce3f6",
			"metadata": {},
			"outputs": [],
			"source": [
				"from google.cloud import storage\n",
				"import pandas as pd\n",
				"import json\n",
				"from langchain.embeddings import VertexAIEmbeddings\n",
				"from langchain.embeddings.base import Embeddings\n",
				"from langchain.llms import VertexAI\n",
				"from langchain.prompts import PromptTemplate\n",
				"from pydantic import BaseModel\n",
				"import vertexai\n",
				"import logging\n",
				"import time\n",
				"import uuid\n",
				"import os\n",
				"from io import BytesIO\n",
				"import logging\n",
				"\n",
				"logging.basicConfig(level=logging.INFO)\n",
				"\n",
				"\n",
				"class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
				"    requests_per_minute: int\n",
				"    num_instances_per_batch: int\n",
				"    mins_per_batch: int = 10 # 10 mins is chosen to get 5*600*10 embeddings (30000)\n",
				"\n",
				"    # Overriding embed_documents method\n",
				"        # Overriding embed_documents method\n",
				"    def embed_documents(self, texts: List[str]) -> List[float]:\n",
				"        \"\"\"Embed documents using the VertexAI API.\n",
				"\n",
				"        Args:\n",
				"            texts: List of documents to embed.\n",
				"\n",
				"        Returns:\n",
				"            List of embeddings.\n",
				"        \"\"\"\n",
				"        limiter = rate_limit(self.requests_per_minute)\n",
				"        results = []\n",
				"        docs = list(texts)\n",
				"\n",
				"        while docs:\n",
				"            # Working in batches because the API accepts maximum 5\n",
				"            # documents per request to get embeddings\n",
				"            head, docs = (\n",
				"                docs[: self.num_instances_per_batch],\n",
				"                docs[self.num_instances_per_batch :],\n",
				"            )\n",
				"            try:\n",
				"                chunk = self.client.get_embeddings(head)\n",
				"                results.extend(chunk)\n",
				"                next(limiter)\n",
				"            except Exception as e:\n",
				"                logging.error(f\"Failed to get embeddings: {e}\")\n",
				"                continue\n",
				"\n",
				"        return [r.values for r in results]\n",
				"\n",
				"                \n",
				"    def embed_and_save_documents(self, texts: List[str], document_ids: List[str], bucket_uri: str, unique_folder_name: str) -> None:\n",
				"        \"\"\"Embed documents and save them to GCS.\n",
				"\n",
				"        Args:\n",
				"            texts: List of documents to embed.\n",
				"            document_ids: List of document IDs.\n",
				"            bucket_uri: URI of the bucket to save to.\n",
				"            unique_folder_name: Unique name for the folder to save to.\n",
				"        \"\"\"\n",
				"        limiter = rate_limit(self.requests_per_minute)\n",
				"        batch_size = self.num_instances_per_batch * self.requests_per_minute * self.mins_per_batch\n",
				"\n",
				"        for i in tqdm(range(0, len(texts), batch_size)):  \n",
				"            try:\n",
				"                embeddings = self.embed_documents(texts[i: i + batch_size])\n",
				"                self.write_embeddings_to_bucket(document_ids[i: i + batch_size], embeddings, bucket_uri, unique_folder_name)\n",
				"\n",
				"                df = pd.DataFrame({\n",
				"                    'document_id': document_ids[i: i + batch_size]\n",
				"                })\n",
				"                pp.pprint(df)\n",
				"                self.write_parquet_to_bucket(bucket_uri, df)\n",
				"\n",
				"                next(limiter)\n",
				"            except Exception as e:\n",
				"                logging.error(f\"Failed to get embeddings: {e}\")\n",
				"                continue\n",
				"\n",
				"\n",
				"    def write_embeddings_to_bucket(self, document_ids: List[str], embeddings: List[float], bucket_uri: str, unique_folder_name: str) -> None:\n",
				"        \"\"\"Write embeddings to a GCS bucket.\n",
				"\n",
				"        Args:\n",
				"            document_ids: List of document IDs.\n",
				"            embeddings: List of embeddings.\n",
				"            bucket_uri: URI of the bucket to write to.\n",
				"            unique_folder_name: Unique name for the folder to write to.\n",
				"        \"\"\"\n",
				"        bucket = storage.Client().bucket(bucket_uri.split('/')[-1])\n",
				"        embeddings_str = \"\".join([json.dumps({\"id\": str(id), \"embedding\": vectors}) + \"\\n\" for id, vectors in zip(document_ids, embeddings)])\n",
				"        bucket.blob(f\"{unique_folder_name}/{uuid.uuid4().hex}.json\").upload_from_string(embeddings_str)\n",
				"\n",
				"\n",
				"    def write_parquet_to_bucket(self, bucket_uri: str, df: pd.DataFrame) -> None:\n",
				"        \"\"\"Write parquet data to a GCS bucket.\n",
				"\n",
				"        Args:\n",
				"            bucket_uri: URI of the bucket to write to.\n",
				"            df: DataFrame to write.\n",
				"        \"\"\"\n",
				"        folder_name = \"embedded_document_ids\"\n",
				"        bucket = storage.Client().bucket(bucket_uri.split('/')[-1])\n",
				"        blob = bucket.blob(f\"{folder_name}/{uuid.uuid4().hex}.parquet\")\n",
				"    \n",
				"        # We need to create a BytesIO object because blob.upload_from_file requires a file-like object\n",
				"        with io.BytesIO() as f:\n",
				"            df.to_parquet(f)\n",
				"            f.seek(0)\n",
				"            blob.upload_from_file(f)\n",
				"\n",
				"\n",
				"    def get_existing_ids(self, bucket_uri: str) -> pd.DataFrame:\n",
				"        \"\"\"Check for existing document ids and vector ids file in GCS and return a dataframe if it exists.\n",
				"\n",
				"        Args:\n",
				"            bucket_uri: URI of the bucket to check.\n",
				"\n",
				"        Returns:\n",
				"            DataFrame of existing ids.\n",
				"        \"\"\"\n",
				"        storage_client = storage.Client()\n",
				"        bucket_name = bucket_uri.split('/')[-1]\n",
				"        bucket = storage_client.bucket(bucket_name)\n",
				"        blobs = bucket.list_blobs(prefix='embedded_document_ids/')\n",
				"\n",
				"        df_list = []\n",
				"        for blob in blobs:\n",
				"            if blob.name.endswith('.parquet'):\n",
				"                byte_stream = BytesIO()\n",
				"                blob.download_to_file(byte_stream)\n",
				"                byte_stream.seek(0)\n",
				"                df = pd.read_parquet(byte_stream)\n",
				"                df_list.append(df)\n",
				"\n",
				"        if df_list:\n",
				"            return pd.concat(df_list)\n",
				"        else:\n",
				"            return pd.DataFrame()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 211,
			"id": "1943d21e-9703-4425-a8fb-c3bcfe70b327",
			"metadata": {},
			"outputs": [],
			"source": [
				"vertexai.init(project=PROJECT_ID, location=REGION)\n",
				"\n",
				"# Text model instance integrated with langChain\n",
				"llm = VertexAI(\n",
				"    model_name=\"text-bison@001\",\n",
				"    max_output_tokens=1024,\n",
				"    temperature=0,\n",
				"    top_p=0.8,\n",
				"    top_k=40,\n",
				"    verbose=True,\n",
				")\n",
				"\n",
				"# Embeddings API integrated with langChain\n",
				"EMBEDDING_QPM = 600\n",
				"EMBEDDING_NUM_BATCH = 5\n",
				"embedding = CustomVertexAIEmbeddings(\n",
				"    requests_per_minute=EMBEDDING_QPM,\n",
				"    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
				"    mins_per_batch = 1\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "daacb3f6-a7de-4685-87b8-1f577ed0cc0b",
			"metadata": {},
			"source": [
				"## Create Indexes & Deploy Endpoints\n",
				"\n",
				"The following steps need to happen to deploy a Matching Engine endpoint:\n",
				"\n",
				"1. Create an index. This class implements the tree-AH algorithm. It reads the embeddings generated from the earlier step in a GCP bucket in the correct format, as described in the [documentation](https://urldefense.com/v3/__https://cloud.google.com/vertex-ai/docs/matching-engine/match-eng-setup/format-structure*data-file-formats).*5Cn__;IyU!!KEc8uF_xo8-al5zF!TFxM5JIZ9RmphkPHNnVGObbMz3_4wX82ie0o_wIrJ31_owKC5da5eEro18u5ec7Nt-gk-kJOCs-Kw0rv$ ",
				"2. Create a Matching Engine Index Endpoint on your VPC Network (Not required).\n",
				"3. Deploy the endpoint by associating it with a deployed Matching Engine Index.\n",
				"\n",
				"**Note**: Depending on your use case, your setup might look different for creating and deploying an index. The main differentiator is whether you want to deploy your index to a public endpoint or to your own Virtual Private Cloud (VPC). If you decide to use a VPC, you can enable Private Service Connect or VPC Peering."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 225,
			"id": "f06139fe-e8af-42d2-b6de-332ee1b2ddc8",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Utility functions to create Index and deploy the index to an Endpoint\n",
				"from datetime import datetime\n",
				"import time\n",
				"import logging\n",
				"import tempfile\n",
				"\n",
				"from google.cloud import aiplatform_v1 as aipv1, storage\n",
				"from google.protobuf import struct_pb2\n",
				"\n",
				"logging.basicConfig(level = logging.INFO)\n",
				"logger = logging.getLogger()\n",
				"\n",
				"class MatchingEngineUtils:\n",
				"    def __init__(self,\n",
				"                 project_id: str,\n",
				"                 region: str,\n",
				"                 index_name: str):\n",
				"        self.project_id = project_id\n",
				"        self.region = region\n",
				"        self.index_name = index_name\n",
				"        self.index_endpoint_name = f\"{self.index_name}_endpoint\"\n",
				"        self.PARENT = f\"projects/{self.project_id}/locations/{self.region}\"\n",
				"\n",
				"        ENDPOINT = f\"{self.region}-aiplatform.googleapis.com\"\n",
				"        # set index client\n",
				"        self.index_client = aipv1.IndexServiceClient(\n",
				"            client_options=dict(api_endpoint=ENDPOINT)\n",
				"        )\n",
				"        # set index endpoint client\n",
				"        self.index_endpoint_client = aipv1.IndexEndpointServiceClient(\n",
				"            client_options=dict(api_endpoint=ENDPOINT)\n",
				"        )\n",
				"\n",
				"    def get_index(self):\n",
				"        # Check if index exists\n",
				"        request = aipv1.ListIndexesRequest(parent=self.PARENT)\n",
				"        page_result = self.index_client.list_indexes(request=request)\n",
				"        indexes = [response.name for response in page_result\n",
				"                   if response.display_name == self.index_name]\n",
				"\n",
				"        if len(indexes) == 0:\n",
				"            return None\n",
				"        else:\n",
				"            index_id = indexes[0]\n",
				"            request = aipv1.GetIndexRequest(name=index_id)\n",
				"            index = self.index_client.get_index(request=request)\n",
				"            return index\n",
				"\n",
				"    def get_index_endpoint(self):\n",
				"        # Check if index endpoint exists\n",
				"        request = aipv1.ListIndexEndpointsRequest(parent=self.PARENT)\n",
				"        page_result = self.index_endpoint_client.list_index_endpoints(request=request)\n",
				"        index_endpoints = [response.name for response in page_result\n",
				"                           if response.display_name == self.index_endpoint_name]\n",
				"\n",
				"        if len(index_endpoints) == 0:\n",
				"            return None\n",
				"        else:\n",
				"            index_endpoint_id = index_endpoints[0]\n",
				"            request = aipv1.GetIndexEndpointRequest(name=index_endpoint_id)\n",
				"            index_endpoint = self.index_endpoint_client.get_index_endpoint(request=request)\n",
				"            return index_endpoint\n",
				"\n",
				"        \n",
				"    def create_index(self,\n",
				"                     embedding_gcs_uri: str,\n",
				"                     dimensions: int,\n",
				"                     description: str,\n",
				"                     index_update_method: str = \"streaming\",\n",
				"                     index_algorithm:str = \"tree-ah\"\n",
				"                     ):\n",
				"        # Get index\n",
				"        index = self.get_index()\n",
				"        # Create index if does not exists\n",
				"        if index:\n",
				"            logger.info(f\"Index {self.index_name} already exists with id {index.name}\")\n",
				"        else:\n",
				"            logger.info(f\"Index {self.index_name} does not exists. Creating index ...\")\n",
				"\n",
				"            if index_update_method == \"streaming\":\n",
				"                index_update_method = aipv1.Index.IndexUpdateMethod.STREAM_UPDATE\n",
				"            else:\n",
				"                index_update_method = aipv1.Index.IndexUpdateMethod.BATCH_UPDATE\n",
				"\n",
				"            treeAhConfig = struct_pb2.Struct(\n",
				"                fields={\n",
				"                    \"leafNodeEmbeddingCount\": struct_pb2.Value(number_value=1000),\n",
				"                    \"fractionLeafNodesToSearch\": struct_pb2.Value(number_value=.05),\n",
				"                }\n",
				"            )\n",
				"            if index_algorithm == \"tree-ah\":\n",
				"                algorithmConfig = struct_pb2.Struct(\n",
				"                    fields={\"treeAhConfig\": struct_pb2.Value(struct_value=treeAhConfig)}\n",
				"                )\n",
				"            else:\n",
				"                algorithmConfig = struct_pb2.Struct(\n",
				"                    fields={\"bruteForceConfig\": struct_pb2.Value(struct_value=struct_pb2.Struct())}\n",
				"                )\n",
				"            config = struct_pb2.Struct(\n",
				"                fields={\n",
				"                    \"dimensions\": struct_pb2.Value(number_value=dimensions),\n",
				"                    \"approximateNeighborsCount\": struct_pb2.Value(number_value=150),\n",
				"                    \"distanceMeasureType\": struct_pb2.Value(string_value=\"DOT_PRODUCT_DISTANCE\"),\n",
				"                    \"algorithmConfig\": struct_pb2.Value(struct_value=algorithmConfig),\n",
				"                    \"shardSize\": struct_pb2.Value(string_value=\"SHARD_SIZE_SMALL\"),\n",
				"                    \"featureNormType\": struct_pb2.Value(string_value=\"UNIT_L2_NORM\")\n",
				"                }\n",
				"            )\n",
				"            metadata = struct_pb2.Struct(\n",
				"                fields={\n",
				"                    \"config\": struct_pb2.Value(struct_value=config),\n",
				"                    \"contentsDeltaUri\": struct_pb2.Value(string_value=embedding_gcs_uri),\n",
				"                }\n",
				"            )\n",
				"\n",
				"            index_request = {\n",
				"                \"display_name\": self.index_name,\n",
				"                \"description\": description,\n",
				"                \"metadata\": struct_pb2.Value(struct_value=metadata),\n",
				"                \"index_update_method\": index_update_method,\n",
				"            }\n",
				"\n",
				"            r = self.index_client.create_index(parent=self.PARENT,\n",
				"                                               index=index_request)\n",
				"            logger.info(f'Creating index with long running operation {r._operation.name}')\n",
				"\n",
				"            # Poll the operation until it's done successfullly.\n",
				"            logging.info(\"Poll the operation to create index ...\")\n",
				"            while True:\n",
				"                if r.done():\n",
				"                    break\n",
				"                time.sleep(60)\n",
				"                print('.', end='')\n",
				"\n",
				"            index = r.result()\n",
				"            logger.info(f\"Index {self.index_name} created with resource name as {index.name}\")\n",
				"\n",
				"        return index\n",
				"\n",
				"\n",
				"    def deploy_index(self,\n",
				"                     machine_type: str = \"e2-standard-2\",\n",
				"                     min_replica_count: int = 2,\n",
				"                     max_replica_count: int = 10,\n",
				"                     network: str = None):\n",
				"        try:\n",
				"            # Get index if exists\n",
				"            index = self.get_index()\n",
				"            if not index:\n",
				"                raise Exception(f\"Index {self.index_name} does not exists. Please create index before deploying.\")\n",
				"\n",
				"            # Get index endpoint if exists\n",
				"            index_endpoint = self.get_index_endpoint()\n",
				"            # Create Index Endpoint if does not exists\n",
				"            if index_endpoint:\n",
				"                logger.info(f\"Index endpoint {self.index_endpoint_name} already exists with resource \" +\n",
				"                            f\"name as {index_endpoint.name} and endpoint domain name as \" +\n",
				"                            f\"{index_endpoint.public_endpoint_domain_name}\")\n",
				"            else:\n",
				"                logger.info(f\"Index endpoint {self.index_endpoint_name} does not exists. Creating index endpoint...\")\n",
				"                index_endpoint_request = {\"display_name\": self.index_endpoint_name}\n",
				"\n",
				"                if network:\n",
				"                    index_endpoint_request[\"network\"] = network\n",
				"                else:\n",
				"                    index_endpoint_request[\"public_endpoint_enabled\"] = True\n",
				"\n",
				"                r = self.index_endpoint_client.create_index_endpoint(\n",
				"                    parent=self.PARENT,\n",
				"                    index_endpoint=index_endpoint_request)\n",
				"                logger.info(f'Deploying index to endpoint with long running operation {r._operation.name}')\n",
				"\n",
				"                logger.info(\"Poll the operation to create index endpoint ...\")\n",
				"                while True:\n",
				"                    if r.done():\n",
				"                        break\n",
				"                    time.sleep(60)\n",
				"                    print('.', end='')\n",
				"\n",
				"                index_endpoint = r.result()\n",
				"                logger.info(f\"Index endpoint {self.index_endpoint_name} created with resource \" +\n",
				"                            f\"name as {index_endpoint.name} and endpoint domain name as \" +\n",
				"                            f\"{index_endpoint.public_endpoint_domain_name}\")\n",
				"        except Exception as e:\n",
				"            logger.error(f\"Failed to create index endpoint {self.index_endpoint_name}\")\n",
				"            raise e\n",
				"\n",
				"        # Deploy Index to endpoint\n",
				"        try:\n",
				"            # Check if index is already deployed to the endpoint\n",
				"            for d_index in index_endpoint.deployed_indexes:\n",
				"                if d_index.index == index.name:\n",
				"                    logger.info(f\"Skipping deploying Index. Index {self.index_name}\" +\n",
				"                                f\"already deployed with id {index.name} to the index endpoint {self.index_endpoint_name}\")\n",
				"                    return index_endpoint\n",
				"\n",
				"            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
				"            deployed_index_id = f\"{self.index_name.replace('-', '_')}_{timestamp}\"\n",
				"            deploy_index = {\n",
				"                \"id\": deployed_index_id,\n",
				"                \"display_name\": deployed_index_id,\n",
				"                \"index\": index.name,\n",
				"                \"dedicated_resources\": {\n",
				"                    \"machine_spec\": {\n",
				"                        \"machine_type\": machine_type,\n",
				"                        },\n",
				"                    \"min_replica_count\": min_replica_count,\n",
				"                    \"max_replica_count\": max_replica_count\n",
				"                    }\n",
				"            }\n",
				"            logger.info(f\"Deploying index with request = {deploy_index}\")\n",
				"            r = self.index_endpoint_client.deploy_index(\n",
				"                index_endpoint=index_endpoint.name,\n",
				"                deployed_index=deploy_index\n",
				"            )\n",
				"\n",
				"            # Poll the operation until it's done successfullly.\n",
				"            logger.info(\"Poll the operation to deploy index ...\")\n",
				"            while True:\n",
				"                if r.done():\n",
				"                    break\n",
				"                time.sleep(60)\n",
				"                print('.', end='')\n",
				"\n",
				"            logger.info(f\"Deployed index {self.index_name} to endpoint {self.index_endpoint_name}\")\n",
				"\n",
				"        except Exception as e:\n",
				"            logger.error(f\"Failed to deploy index {self.index_name} to the index endpoint {self.index_endpoint_name}\")\n",
				"            raise e\n",
				"\n",
				"        return index_endpoint\n",
				"\n",
				"    def get_index_and_endpoint(self):\n",
				"        # Get index id if exists\n",
				"        index = self.get_index()\n",
				"        index_id = index.name if index else ''\n",
				"\n",
				"        # Get index endpoint id if exists\n",
				"        index_endpoint = self.get_index_endpoint()\n",
				"        index_endpoint_id = index_endpoint.name if index_endpoint else ''\n",
				"        \n",
				"\n",
				"        return index_id, index_endpoint_id\n",
				"\n",
				"    def delete_index(self):\n",
				"        # Check if index exists\n",
				"        index = self.get_index()\n",
				"\n",
				"        # create index if does not exists\n",
				"        if index:\n",
				"            # Delete index\n",
				"            index_id = index.name\n",
				"            logger.info(f\"Deleting Index {self.index_name} with id {index_id}\")\n",
				"            self.index_client.delete_index(name=index_id)\n",
				"        else:\n",
				"            raise Exception(\"Index {index_name} does not exists.\")\n",
				"\n",
				"    def delete_index_endpoint(self):\n",
				"        # Check if index endpoint exists\n",
				"        index_endpoint = self.get_index_endpoint()\n",
				"\n",
				"        # Create Index Endpoint if does not exists\n",
				"        if index_endpoint:\n",
				"            logger.info(f\"Index endpoint {self.index_endpoint_name}  exists with resource \" +\n",
				"                        f\"name as {index_endpoint.name} and endpoint domain name as \" +\n",
				"                        f\"{index_endpoint.public_endpoint_domain_name}\")\n",
				"\n",
				"            index_endpoint_id = index_endpoint.name\n",
				"            index_endpoint = self.index_endpoint_client.get_index_endpoint(name=index_endpoint_id)\n",
				"            # Undeploy existing indexes\n",
				"            for d_index in index_endpoint.deployed_indexes:\n",
				"                logger.info(f\"Undeploying index with id {d_index.id} from Index endpoint {self.index_endpoint_name}\")\n",
				"                request = aipv1.UndeployIndexRequest(\n",
				"                    index_endpoint=index_endpoint_id,\n",
				"                    deployed_index_id=d_index.id)\n",
				"                r = self.index_endpoint_client.undeploy_index(request=request)\n",
				"                response = r.result()\n",
				"                logger.info(response)\n",
				"\n",
				"            # Delete index endpoint\n",
				"            logger.info(f\"Deleting Index endpoint {self.index_endpoint_name} with id {index_endpoint_id}\")\n",
				"            self.index_endpoint_client.delete_index_endpoint(name=index_endpoint_id)\n",
				"        else:\n",
				"            raise Exception(f\"Index endpoint {self.index_endpoint_name} does not exists.\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 178,
			"id": "b547b161-9c01-4fac-92c4-cf1b886a97a8",
			"metadata": {},
			"outputs": [{
				"data": {
					"text/html": [
						"<div>\n",
						"<style scoped>\n",
						"    .dataframe tbody tr th:only-of-type {\n",
						"        vertical-align: middle;\n",
						"    }\n",
						"\n",
						"    .dataframe tbody tr th {\n",
						"        vertical-align: top;\n",
						"    }\n",
						"\n",
						"    .dataframe thead th {\n",
						"        text-align: right;\n",
						"    }\n",
						"</style>\n",
						"<table border=\"1\" class=\"dataframe\">\n",
						"  <thead>\n",
						"    <tr style=\"text-align: right;\">\n",
						"      <th></th>\n",
						"      <th>legal_entity_identifier</th>\n",
						"      <th>company_name_latin_alphabet</th>\n",
						"    </tr>\n",
						"  </thead>\n",
						"  <tbody>\n",
						"    <tr>\n",
						"      <th>0</th>\n",
						"      <td>CS006111</td>\n",
						"      <td>PERTH &amp; KINROSS DISABILITY SPORT - SCIO</td>\n",
						"    </tr>\n",
						"    <tr>\n",
						"      <th>1</th>\n",
						"      <td>CE024877</td>\n",
						"      <td>THE WASHING MACHINE PROJECT FOUNDATION</td>\n",
						"    </tr>\n",
						"    <tr>\n",
						"      <th>2</th>\n",
						"      <td>CE023261</td>\n",
						"      <td>THE SPARKLE FOUNDATION UK</td>\n",
						"    </tr>\n",
						"    <tr>\n",
						"      <th>3</th>\n",
						"      <td>CS000485</td>\n",
						"      <td>BORDER GROUP OF RIDING FOR THE DISABLED (SCIO)</td>\n",
						"    </tr>\n",
						"    <tr>\n",
						"      <th>4</th>\n",
						"      <td>CS001757</td>\n",
						"      <td>ABOYNE &amp; DISTRICT MEN'S SHED</td>\n",
						"    </tr>\n",
						"  </tbody>\n",
						"</table>\n",
						"</div>"
					],
					"text/plain": [
						"  legal_entity_identifier                     company_name_latin_alphabet\n",
						"0                CS006111         PERTH & KINROSS DISABILITY SPORT - SCIO\n",
						"1                CE024877          THE WASHING MACHINE PROJECT FOUNDATION\n",
						"2                CE023261                       THE SPARKLE FOUNDATION UK\n",
						"3                CS000485  BORDER GROUP OF RIDING FOR THE DISABLED (SCIO)\n",
						"4                CS001757                    ABOYNE & DISTRICT MEN'S SHED"
					]
				},
				"execution_count": 178,
				"metadata": {},
				"output_type": "execute_result"
			}],
			"source": [
				"# Query your reference data and put it in a dataframe\n",
				"# Update Query\n",
				"# Limit to 100,000 records for demo purposes\n",
				"reference_query =  'SELECT legal_entity_identifier, company_name_latin_alphabet FROM `whisper-test-378918.fintech_dummy_data.bvd_embedding_demo` LIMIT 100000'\n",
				"\n",
				"reference_data = bigquery_fetcher.query_to_df(reference_query)\n",
				"reference_data.head()\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 215,
			"id": "ea47ff77-b38b-437e-a301-f82da75f6422",
			"metadata": {},
			"outputs": [{
				"name": "stderr",
				"output_type": "stream",
				"text": [
					"0it [00:00, ?it/s]\n"
				]
			}],
			"source": [
				"# Create embeddings and write them to GCS\n",
				"# Update column name if appropriate\n",
				"# This will take a while # of records / (batch size * rate limit) \n",
				"# 100,000/(5*600) = 33.33 minutes\n",
				"\n",
				"EMBEDDINGS_FOLDER_NAME = 'embeddings_batch_test'  # Update this if needed\n",
				"\n",
				"# Fetch existing document ids from the GCS bucket\n",
				"existing_ids_df = embedding.get_existing_ids(BUCKET_URI)\n",
				"\n",
				"# Check if there are any existing ids, if yes, filter out those ids from the data to be processed\n",
				"if not existing_ids_df.empty:\n",
				"    filtered_reference_data = reference_data.loc[~reference_data.legal_entity_identifier.isin(existing_ids_df.document_id)]\n",
				"    # Updating document_list and document_ids post filtering\n",
				"    document_list = filtered_reference_data.company_name_latin_alphabet.to_list()\n",
				"    document_ids = filtered_reference_data.legal_entity_identifier.to_list()\n",
				"else:\n",
				"    document_list = reference_data.company_name_latin_alphabet.to_list()\n",
				"    embeddings = embedding.embed_documents(document_list)\n",
				"\n",
				"# This initiates the embedding and saving process\n",
				"embedding.embed_and_save_documents(document_list, document_ids, BUCKET_URI, EMBEDDINGS_FOLDER_NAME)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 217,
			"id": "1d6cc924-4c02-46db-bc47-85dee6646a49",
			"metadata": {},
			"outputs": [],
			"source": [
				"embeddings_gcs_uri = f\"{BUCKET_URI}/{EMBEDDINGS_FOLDER_NAME}/\""
			]
		},
		{
			"cell_type": "markdown",
			"id": "634af319-42a0-4142-8e04-8f31d3e4e975",
			"metadata": {},
			"source": [
				"# Build Matching Engine Index"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 218,
			"id": "6cde8575-8631-4b65-82da-23efb23ba30c",
			"metadata": {},
			"outputs": [{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:root:Index fintech_entity_resolution_demo_l2 does not exists. Creating index ...\n",
						"INFO:root:Creating index with long running operation projects/209480788681/locations/us-central1/indexes/8846820090667073536/operations/1487414038655664128\n",
						"INFO:root:Poll the operation to create index ...\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"......................................................"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:root:Index fintech_entity_resolution_demo_l2 created with resource name as projects/209480788681/locations/us-central1/indexes/8846820090667073536\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				}
			],
			"source": [
				"# This will take a while\n",
				"ME_INDEX_NAME = 'fintech_entity_resolution_demo'\n",
				"mengine = MatchingEngineUtils(PROJECT_ID, REGION, ME_INDEX_NAME)\n",
				"\n",
				"reference_data_index = mengine.create_index(embeddings_gcs_uri,\n",
				"                     dimensions=768, # number of embedding features len(embeddings[0]), \n",
				"                     description=\"Fintech Demo\",\n",
				"                     index_update_method=\"batch\",\n",
				"                     index_algorithm=\"tree-ah\"\n",
				"                    )\n",
				"    \n",
				"# ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 224,
			"id": "16b6c7d1-d03b-4c4e-bb22-001d49309bc4",
			"metadata": {},
			"outputs": [{
				"data": {
					"text/plain": [
						"name: \"projects/209480788681/locations/us-central1/indexes/7670149134900264960\"\n",
						"display_name: \"fintech_demo\"\n",
						"metadata_schema_uri: \"gs://google-cloud-aiplatform/schema/matchingengine/metadata/nearest_neighbor_search_1.0.0.yaml\"\n",
						"metadata {\n",
						"  struct_value {\n",
						"    fields {\n",
						"      key: \"config\"\n",
						"      value {\n",
						"        struct_value {\n",
						"          fields {\n",
						"            key: \"algorithmConfig\"\n",
						"            value {\n",
						"              struct_value {\n",
						"                fields {\n",
						"                  key: \"treeAhConfig\"\n",
						"                  value {\n",
						"                    struct_value {\n",
						"                      fields {\n",
						"                        key: \"leafNodeEmbeddingCount\"\n",
						"                        value {\n",
						"                          string_value: \"500\"\n",
						"                        }\n",
						"                      }\n",
						"                      fields {\n",
						"                        key: \"leafNodesToSearchPercent\"\n",
						"                        value {\n",
						"                          number_value: 80.0\n",
						"                        }\n",
						"                      }\n",
						"                    }\n",
						"                  }\n",
						"                }\n",
						"              }\n",
						"            }\n",
						"          }\n",
						"          fields {\n",
						"            key: \"approximateNeighborsCount\"\n",
						"            value {\n",
						"              number_value: 150.0\n",
						"            }\n",
						"          }\n",
						"          fields {\n",
						"            key: \"dimensions\"\n",
						"            value {\n",
						"              number_value: 768.0\n",
						"            }\n",
						"          }\n",
						"          fields {\n",
						"            key: \"distanceMeasureType\"\n",
						"            value {\n",
						"              string_value: \"DOT_PRODUCT_DISTANCE\"\n",
						"            }\n",
						"          }\n",
						"          fields {\n",
						"            key: \"shardSize\"\n",
						"            value {\n",
						"              string_value: \"SHARD_SIZE_MEDIUM\"\n",
						"            }\n",
						"          }\n",
						"        }\n",
						"      }\n",
						"    }\n",
						"  }\n",
						"}\n",
						"deployed_indexes {\n",
						"  index_endpoint: \"projects/209480788681/locations/us-central1/indexEndpoints/8356948078074265600\"\n",
						"  deployed_index_id: \"fintech_demo_endpoint_index_id\"\n",
						"}\n",
						"etag: \"AMEw9yNVF2i3AXXZE5wUgWYcAqeVlHEKHietWPBRsWN6Z3lhDrUZbzqSimcksU--KfI=\"\n",
						"create_time {\n",
						"  seconds: 1687627347\n",
						"  nanos: 221942000\n",
						"}\n",
						"update_time {\n",
						"  seconds: 1687631907\n",
						"  nanos: 229795000\n",
						"}\n",
						"index_stats {\n",
						"  vectors_count: 149740\n",
						"  shards_count: 1\n",
						"}\n",
						"index_update_method: BATCH_UPDATE"
					]
				},
				"execution_count": 224,
				"metadata": {},
				"output_type": "execute_result"
			}],
			"source": [
				"ME_INDEX_NAME = 'fintech_demo'\n",
				"mengine = MatchingEngineUtils(PROJECT_ID, REGION, ME_INDEX_NAME)\n",
				"mengine.get_index()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 86,
			"id": "a1bf4e3b-7d87-45ce-9780-c74ff1e75894",
			"metadata": {},
			"outputs": [],
			"source": [
				"import requests\n",
				"\n",
				"metadata_server_url = \"https://urldefense.com/v3/__http://metadata.google.internal/computeMetadata/v1/project/numeric-project-id*5C*22*5Cn*22__;JSUlJQ!!KEc8uF_xo8-al5zF!TFxM5JIZ9RmphkPHNnVGObbMz3_4wX82ie0o_wIrJ31_owKC5da5eEro18u5ec7Nt-gk-kJOCoYzQzKU$ ",
				"metadata_headers = {\"Metadata-Flavor\": \"Google\"}\n",
				"\n",
				"response = requests.get(metadata_server_url, headers=metadata_headers)\n",
				"project_number = response.content.decode()\n",
				"\n",
				"VPC_NAME = 'text-matching-engine-test'\n",
				"full_network = f\"projects/{project_number}/global/networks/{VPC_NAME}\"\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 87,
			"id": "517d480e-fea9-4cb0-94d4-8618d109966f",
			"metadata": {},
			"outputs": [{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:root:Index endpoint fintech_entity_resolution_demo_endpoint does not exists. Creating index endpoint...\n",
						"INFO:root:Deploying index to endpoint with long running operation projects/209480788681/locations/us-central1/indexEndpoints/3086716167260209152/operations/2016569402685652992\n",
						"INFO:root:Poll the operation to create index endpoint ...\n",
						"INFO:root:Index endpoint fintech_entity_resolution_demo_endpoint created with resource name as projects/209480788681/locations/us-central1/indexEndpoints/3086716167260209152 and endpoint domain name as \n",
						"INFO:root:Deploying index with request = {'id': 'fintech_entity_resolution_demo_20230711010839', 'display_name': 'fintech_entity_resolution_demo_20230711010839', 'index': 'projects/209480788681/locations/us-central1/indexes/1991778507855757312', 'dedicated_resources': {'machine_spec': {'machine_type': 'e2-standard-16'}, 'min_replica_count': 2, 'max_replica_count': 10}}\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:root:Poll the operation to deploy index ...\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"...."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:root:Deployed index fintech_entity_resolution_demo to endpoint fintech_entity_resolution_demo_endpoint\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"."
					]
				}
			],
			"source": [
				"# Deploy the index\n",
				"index_endpoint = mengine.deploy_index(\n",
				"                     machine_type = \"e2-standard-16\",\n",
				"                     min_replica_count = 2,\n",
				"                     max_replica_count = 10,\n",
				"                     network = full_network\n",
				"                    )"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 32,
			"id": "80d022da-83f6-4a7e-9e82-cdaf5464fde3",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Class for retrieving ANN results from Matching Engine\n",
				"# Requires an index endpoint to be deployed along with deployed index id\n",
				"class MatchingEngineRetriever():\n",
				"    def __init__(\n",
				"            self,\n",
				"            index_endpoint: str, \n",
				"            embedding: Embeddings \n",
				"            ):\n",
				"        from google.cloud import aiplatform\n",
				"        self.index_endpoint = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_name=index_endpoint)\n",
				"        self.deployed_index_id = self.index_endpoint.deployed_indexes[-1].id #returns the latest deploy index\n",
				"        self.embedding = embedding\n",
				"    \n",
				"    \n",
				"    def retrieve_results(self, num_neighbors: int, queries: list) -> List[List]:\n",
				"        embedding_queries = self.embedding.embed_documents(queries)\n",
				"        response = self.index_endpoint.match(\n",
				"            deployed_index_id=self.deployed_index_id,\n",
				"            queries=embedding_queries,\n",
				"            num_neighbors=num_neighbors,\n",
				"        )\n",
				"    \n",
				"        return response\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 33,
			"id": "e3c5eeac-0dda-4047-b24d-e2b0349a9556",
			"metadata": {},
			"outputs": [],
			"source": [
				"me_retriver = MatchingEngineRetriever(\n",
				"    index_endpoint=ME_INDEX_ENDPOINT_ID, \n",
				"    embedding =embedding )"
			]
		},
		{
			"cell_type": "markdown",
			"id": "114c2f7f-ba41-45ed-806f-f08f1f93636a",
			"metadata": {},
			"source": [
				"## Entity Resolution"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 34,
			"id": "90c999d7-1d62-4a6a-a614-5f9b2352d394",
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "code",
			"execution_count": 45,
			"id": "842f06df-a2b2-48cd-840e-cde3994eac02",
			"metadata": {},
			"outputs": [],
			"source": [
				"from tqdm.auto import tqdm\n",
				"from dataclasses import dataclass\n",
				"import uuid\n",
				"\n",
				"@dataclass\n",
				"class EntityResolution:\n",
				"    uuid: str\n",
				"    document_id: str\n",
				"    entity_name: str\n",
				"    nearest_neighbor_id: int\n",
				"    nearest_neighbor: str\n",
				"    distance_score: float\n",
				"    prompt: str\n",
				"    classification: str\n",
				"\n",
				"\n",
				"class EntityResolutionSystem:\n",
				"    def __init__(self, retriever: MatchingEngineRetriever, requests_per_minute: int, canonical_df: pd.DataFrame):\n",
				"        self.me_retriver = retriever\n",
				"        self._llm = VertexAI(\n",
				"            model_name=\"text-bison@001\",\n",
				"            max_output_tokens=1024,\n",
				"            temperature=0.2,\n",
				"            top_p=0.8,\n",
				"            top_k=40,\n",
				"            verbose=True,\n",
				"        )\n",
				"        self.requests_per_minute = requests_per_minute\n",
				"        self.canonical_df = canonical_df\n",
				"        self._llm_chain = self._prepare_prompt()\n",
				"        \n",
				"    \n",
				"    def _prepare_prompt(self) -> LLMChain:\n",
				"        \"\"\"\n",
				"        Prepare the prompt to be used for entity matching\n",
				"        \"\"\"\n",
				"        preamble = '''\n",
				"        Given the two entities separated by the [SEP] token, determine whether they represent the same entity or different ones. \n",
				"        If they represent the same entity, the output should be 'match'. \n",
				"        If they represent different entities, the output should be 'no match'.\n",
				"        Remember, your task is to evaluate the entities and classify them as either 'match' or 'no match' based on their similarity.\n",
				"        Remember, this task demands an in-depth evaluation and a keen sense of judgement.\n",
				"        Here are some examples:\n",
				"        '''\n",
				"\n",
				"        from langchain.prompts.few_shot import FewShotPromptTemplate\n",
				"        from langchain.prompts.prompt import PromptTemplate\n",
				"\n",
				"\n",
				"        examples = [\n",
				"          {\n",
				"            \"input\": \"FIT ACTIVE BODY LTD [SEP] FITNESS INCORPORATED LTD \",\n",
				"            \"output\": 'no match'\n",
				"          }\n",
				"        ]\n",
				"\n",
				"        suffix = '''\n",
				"            Given the following pair of entities:\n",
				"            Input: {input} \n",
				"            output:'''\n",
				"\n",
				"        example_formatter_template = \"\"\"\n",
				"        input: {input} \n",
				"        output: {output}\"\"\"\n",
				"\n",
				"        example_prompt = PromptTemplate(input_variables=[\"input\", \"output\"], template=example_formatter_template)\n",
				"\n",
				"\n",
				"        prompt = FewShotPromptTemplate(\n",
				"            examples=examples, \n",
				"            example_prompt=example_prompt,\n",
				"            prefix=preamble,\n",
				"            suffix=suffix, \n",
				"            input_variables=[\"input\"]\n",
				"        )\n",
				"        \n",
				"        llm_chain = LLMChain(prompt=prompt, llm=self._llm)\n",
				"\n",
				"        return llm_chain\n",
				"        \n",
				"        \n",
				"    def _filter_right_entities (self, rights_ids) -> List[str]:\n",
				"        \"\"\"\n",
				"        Filter and return right entities from canonical_df based on provided ids\n",
				"        \"\"\"\n",
				"        # Get a DataFrame of the rows where 'key' is in the list 'keys'\n",
				"        matched_rows_df = self.canonical_df[self.canonical_df.iloc[:,0].isin(rights_ids)]\n",
				"\n",
				"        # Get the column name\n",
				"        column_name = matched_rows_df.columns[-1]\n",
				"\n",
				"        # Extract the 'value' column from the matched rows as a list and prefix each value with column name\n",
				"        # return [f'{column_name}={name}' for name in matched_rows_df.iloc[:,-1].tolist()]\n",
				"        return matched_rows_df.iloc[:,-1].tolist()\n",
				"\n",
				"\n",
				"    def _serialize_pair(entry1, entry2):\n",
				"        CLS = \"[CLS]entity_name=\"\n",
				"        SEP = \"[SEP]\"\n",
				"\n",
				"        serialized_entry1 = serialize(entry1)\n",
				"        serialized_entry2 = serialize(entry2)\n",
				"\n",
				"        return f\"{CLS} {serialized_entry1} {SEP} {CLS} {serialized_entry2}\"\n",
				"    \n",
				"\n",
				"    def match_entities(self, ids: list, queries: List, num_neighbors: int=5, distance_threshold: int=0) -> List[EntityResolution]:\n",
				"        \"\"\"\n",
				"        Match entities and return a list of resolved entities\n",
				"        \"\"\"\n",
				"        entities_resolved = []\n",
				"        limiter = rate_limit(self.requests_per_minute)\n",
				"        nearest_neighbors_results = self.me_retriver.retrieve_results(num_neighbors=num_neighbors, queries=queries)\n",
				"\n",
				"        # create a tqdm progress bar with a total and a description\n",
				"        progress_bar = tqdm(total=len(ids), desc=\"Matching entities\", dynamic_ncols=True)\n",
				"\n",
				"        #all 3 lists should have the same lenght\n",
				"        for id, query, nearest_neighbors in zip(ids, queries, nearest_neighbors_results):\n",
				"            left_entity = str(query.entity_name)\n",
				"            left_id = id\n",
				"\n",
				"            right_ids = [neighbor.id for neighbor in nearest_neighbors if neighbor.distance >= distance_threshold]\n",
				"            right_entities = self._filter_right_entities(right_ids)\n",
				"            nearest_neighbors_distance_scores = [neighbor.distance for neighbor in nearest_neighbors if neighbor.distance >= distance_threshold]\n",
				"            \n",
				"            #Loop through the top N neighbors for classiciation\n",
				"            for right_id, right_entity, nearest_neighbor_distance_score in zip(right_ids,right_entities, nearest_neighbors_distance_scores):\n",
				"            # prepare a unique identifier for each prompt\n",
				"                entity_pair_to_match = left_entity + \" [SEP] \" + right_entity\n",
				"                # entity_pair_to_match = _serialize_pair(left_entity, right_entity)\n",
				"\n",
				"                #Update the dataclass\n",
				"                entities_resolved.append(EntityResolution(\n",
				"                    uuid = str(uuid.uuid4()),\n",
				"                    document_id = left_id,\n",
				"                    entity_name = left_entity,\n",
				"                    nearest_neighbor_id = right_id,\n",
				"                    nearest_neighbor = right_entity,\n",
				"                    distance_score = nearest_neighbor_distance_score,\n",
				"                    prompt = self._llm_chain.prompt.format(input=entity_pair_to_match),\n",
				"                    classification = self._llm_chain.run(entity_pair_to_match)\n",
				"                ))\n",
				"                next(limiter)\n",
				"                if len(entities_resolved) > 50:\n",
				"                    self._write_results_to_bq(entities_resolved)\n",
				"                    entities_resolved = []\n",
				"\n",
				"            # update the progress bar\n",
				"            progress_bar.update()\n",
				"\n",
				"        # close the progress bar\n",
				"        progress_bar.close()\n",
				"\n",
				"        return entities_resolved\n",
				"\n",
				"    def _write_results_to_bq(self, entities_resolved: List):\n",
				"    # Define the table name in the format 'project_id.dataset_id.table_id'\n",
				"        from dataclasses import asdict\n",
				"        import pandas_gbq\n",
				"        entities_resolved_dict = [asdict(resolved_entity) for resolved_entity in entities_resolved]\n",
				"        df = pd.DataFrame(entities_resolved_dict)\n",
				"\n",
				"        project_id = 'whisper-test-378918'\n",
				"        dataset_id = 'fintech_dummy_data'\n",
				"        table_id = 'entity_reconciliation_demo_new'\n",
				"\n",
				"        table_name = f'{project_id}.{dataset_id}.{table_id}'\n",
				"        pandas_gbq.to_gbq(df, table_id, project_id=project_id, if_exists='append')\n",
				"    \n",
				"    \n",
				"\n",
				"    "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "432be59e-104b-4df2-aaac-b98a29b35a21",
			"metadata": {},
			"outputs": [{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Waiting\n",
						"............."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\n",
						"\n",
						"Matching entities:   0%|          | 0/99 [00:00<?, ?it/s]\u001b[A\u001b[A"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Waiting\n",
						"...."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\n",
						"\n",
						"Matching entities:   1%|          | 1/99 [00:05<08:32,  5.23s/it]\u001b[A\u001b[A"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"...."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\n",
						"\n",
						"Matching entities:   2%|▏         | 2/99 [00:11<09:23,  5.81s/it]\u001b[A\u001b[A"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"....."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\n",
						"\n",
						"Matching entities:   3%|▎         | 3/99 [00:16<09:02,  5.65s/it]\u001b[A\u001b[A"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"....."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\n",
						"\n",
						"Matching entities:   4%|▍         | 4/99 [00:22<08:49,  5.57s/it]\u001b[A\u001b[A"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"....."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\n",
						"\n",
						"Matching entities:   5%|▌         | 5/99 [00:27<08:40,  5.53s/it]\u001b[A\u001b[A"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"...."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\n",
						"\n",
						"Matching entities:   6%|▌         | 6/99 [00:34<09:11,  5.93s/it]\u001b[A\u001b[A"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"...."
					]
				}
			],
			"source": [
				"entity_matcher = EntityResolutionSystem(retriever=me_retriver, requests_per_minute=55, canonical_df=canonical_data)\n",
				"\n",
				"resolved_entities = entity_matcher.match_entities(\n",
				"    ids = [entity_extraction.document_id for entity_extraction in extractions_list],\n",
				"    queries = [entity_extraction.entity for entity_extraction in extractions_list]\n",
				"    )\n",
				"\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 593,
			"id": "87862508-f519-4e4b-905c-8d22b302ed51",
			"metadata": {},
			"outputs": [{
				"name": "stderr",
				"output_type": "stream",
				"text": [
					"\n",
					"\n",
					"\n",
					"\n",
					"25 out of 25 rows loaded.00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[AINFO:pandas_gbq.gbq:\n",
					"100%|██████████| 1/1 [00:00<00:00, 917.59it/s]\n"
				]
			}],
			"source": [
				"\n",
				"import pandas_gbq\n",
				"def write_results_to_bq(df, project_id, dataset_id, table_id):\n",
				"    # Define the table name in the format 'project_id.dataset_id.table_id'\n",
				"    table_name = f'{project_id}.{dataset_id}.{table_id}'\n",
				"    pandas_gbq.to_gbq(df, table_name, project_id=project_id, if_exists='replace')\n",
				"    \n",
				"    \n",
				"write_results_to_bq(df, project_id, dataset_id, table_id)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 729,
			"id": "97a3000c-3d7b-466e-9542-5f27d2797f9e",
			"metadata": {},
			"outputs": [{
				"data": {
					"text/plain": [
						"25"
					]
				},
				"execution_count": 729,
				"metadata": {},
				"output_type": "execute_result"
			}],
			"source": [
				"len(resolved_entities)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 581,
			"id": "9fcb9720-ce0a-458f-8915-fa5042506e4b",
			"metadata": {},
			"outputs": [{
				"name": "stdout",
				"output_type": "stream",
				"text": [
					"'l'\n"
				]
			}],
			"source": [
				"pp.pprint('l')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "878acf26-9242-4d88-a66a-64d2f654ac59",
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"environment": {
			"kernel": "python3",
			"name": "common-cpu.m109",
			"type": "gcloud",
			"uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
		},
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.10.11"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}